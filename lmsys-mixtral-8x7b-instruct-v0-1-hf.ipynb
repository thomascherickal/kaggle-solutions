{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b984ef93",
   "metadata": {
    "papermill": {
     "duration": 0.008744,
     "end_time": "2024-05-30T16:43:10.605024",
     "exception": false,
     "start_time": "2024-05-30T16:43:10.596280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Published on May 05, 2024. By Mar√≠lia Prata, mpwolke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d33dbe",
   "metadata": {
    "papermill": {
     "duration": 0.008118,
     "end_time": "2024-05-30T16:43:10.621598",
     "exception": false,
     "start_time": "2024-05-30T16:43:10.613480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://cdn-thumbnails.huggingface.co/social-thumbnails/models/mistralai/Mixtral-8x7B-Instruct-v0.1.png)HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf56f61a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T16:43:10.639346Z",
     "iopub.status.busy": "2024-05-30T16:43:10.638998Z",
     "iopub.status.idle": "2024-05-30T16:43:47.063563Z",
     "shell.execute_reply": "2024-05-30T16:43:47.062335Z"
    },
    "papermill": {
     "duration": 36.436462,
     "end_time": "2024-05-30T16:43:47.066153",
     "exception": false,
     "start_time": "2024-05-30T16:43:10.629691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154615d",
   "metadata": {
    "papermill": {
     "duration": 0.008186,
     "end_time": "2024-05-30T16:43:47.082928",
     "exception": false,
     "start_time": "2024-05-30T16:43:47.074742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Mixtral 8x7b-instruct\n",
    "\n",
    "Mixtral pytorch/8x7b-instruct-v0.1-hf\n",
    "\n",
    "That will take more than 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76792cbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T16:43:47.101626Z",
     "iopub.status.busy": "2024-05-30T16:43:47.101293Z",
     "iopub.status.idle": "2024-05-30T16:54:09.337117Z",
     "shell.execute_reply": "2024-05-30T16:54:09.336077Z"
    },
    "papermill": {
     "duration": 622.247891,
     "end_time": "2024-05-30T16:54:09.339526",
     "exception": false,
     "start_time": "2024-05-30T16:43:47.091635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19da2afba6b24c2fbf5ef66a936dde70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# To prevent GPU memory overflow in Mixtral8x7b\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "config.gradient_checkpointing = True\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    "    quantization_config=quantization_config,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907e6a7",
   "metadata": {
    "papermill": {
     "duration": 0.008018,
     "end_time": "2024-05-30T16:54:09.356314",
     "exception": false,
     "start_time": "2024-05-30T16:54:09.348296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#I reduced number of rows to get the answers on the last snippet.\n",
    "\n",
    "Otherwise, it would keep running till 1000 and I don't have enough GPU to perform that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dce3b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T16:54:09.374816Z",
     "iopub.status.busy": "2024-05-30T16:54:09.374346Z",
     "iopub.status.idle": "2024-05-30T16:54:10.205173Z",
     "shell.execute_reply": "2024-05-30T16:54:10.204233Z"
    },
    "papermill": {
     "duration": 0.843165,
     "end_time": "2024-05-30T16:54:10.207992",
     "exception": false,
     "start_time": "2024-05-30T16:54:09.364827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "1  53567           koala-13b  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "PRIVATE = True\n",
    "\n",
    "\n",
    "#pd.set_option('display.max_colwidth', 1000)\n",
    "df = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\", nrows=10)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39b70f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T16:54:10.227146Z",
     "iopub.status.busy": "2024-05-30T16:54:10.226867Z",
     "iopub.status.idle": "2024-05-30T16:54:10.239594Z",
     "shell.execute_reply": "2024-05-30T16:54:10.238733Z"
    },
    "papermill": {
     "duration": 0.0241,
     "end_time": "2024-05-30T16:54:10.241574",
     "exception": false,
     "start_time": "2024-05-30T16:54:10.217474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(df) < 5:\n",
    "    df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "    PRIVATE = False\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659f2b1",
   "metadata": {
    "papermill": {
     "duration": 0.008464,
     "end_time": "2024-05-30T16:54:10.258604",
     "exception": false,
     "start_time": "2024-05-30T16:54:10.250140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#The original Dataset is (id/problem/answer) AIMO (AI Math Olympiad progress prize)\n",
    "\n",
    "I replaced ALL problem and answer to PROMPT and RESPONSE_A.\n",
    "\n",
    "Though the program is struggling ('response_a' is not defined) it's returning some answers.\n",
    "\n",
    "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc0c974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T16:54:10.278853Z",
     "iopub.status.busy": "2024-05-30T16:54:10.278578Z",
     "iopub.status.idle": "2024-05-30T16:54:10.283182Z",
     "shell.execute_reply": "2024-05-30T16:54:10.282322Z"
    },
    "papermill": {
     "duration": 0.017296,
     "end_time": "2024-05-30T16:54:10.285013",
     "exception": false,
     "start_time": "2024-05-30T16:54:10.267717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_prompt(problem):\n",
    "    \n",
    "    return f\"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\n",
    "### Instruction:\\n{prompt}\\n\\n\n",
    "### Response: Let's think step by step. The final response should be a single number in the last line of your response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5d2e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T16:54:10.303966Z",
     "iopub.status.busy": "2024-05-30T16:54:10.303688Z",
     "iopub.status.idle": "2024-05-30T16:54:10.307375Z",
     "shell.execute_reply": "2024-05-30T16:54:10.306557Z"
    },
    "papermill": {
     "duration": 0.015235,
     "end_time": "2024-05-30T16:54:10.309188",
     "exception": false,
     "start_time": "2024-05-30T16:54:10.293953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab18687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T16:54:10.327723Z",
     "iopub.status.busy": "2024-05-30T16:54:10.327463Z",
     "iopub.status.idle": "2024-05-30T16:54:10.332727Z",
     "shell.execute_reply": "2024-05-30T16:54:10.331938Z"
    },
    "papermill": {
     "duration": 0.01657,
     "end_time": "2024-05-30T16:54:10.334512",
     "exception": false,
     "start_time": "2024-05-30T16:54:10.317942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_parse(response_a):  #Original was: def naive_parse(answer)\n",
    "    out = []\n",
    "    start = False\n",
    "    end = False\n",
    "    for l in reversed(list(response_a)):\n",
    "        if l in '0123456789' and not end:\n",
    "            start = True\n",
    "            out.append(l)\n",
    "        else:\n",
    "            if start:\n",
    "                end = True\n",
    "        \n",
    "    out = reversed(out)\n",
    "    return int(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3227d7",
   "metadata": {
    "papermill": {
     "duration": 0.008604,
     "end_time": "2024-05-30T16:54:10.351754",
     "exception": false,
     "start_time": "2024-05-30T16:54:10.343150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Since I reduced data lenght (10) I hope to end this Notebook without stopping it\n",
    "\n",
    "Below, I changed problem to prompt and answer to response_a Original was 1000% I reduced to 10% trying to stop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a68694",
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-05-30T16:54:10.370282Z",
     "iopub.status.busy": "2024-05-30T16:54:10.369987Z",
     "iopub.status.idle": "2024-05-30T17:05:18.793053Z",
     "shell.execute_reply": "2024-05-30T17:05:18.792036Z"
    },
    "papermill": {
     "duration": 668.435894,
     "end_time": "2024-05-30T17:05:18.796459",
     "exception": false,
     "start_time": "2024-05-30T16:54:10.360565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]2024-05-30 16:54:17.922378: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-30 16:54:17.922483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-30 16:54:18.028891: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      " 10%|‚ñà         | 1/10 [01:07<10:06, 67.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "As for the moral question of having a certain percentage of females in managerial positions, it's important to recognize that equal representation is a matter of fairness and opportunity rather than a moral issue. Aiming for gender balance in leadership roles can help ensure that diverse perspectives are considered and that everyone has an equal chance to succeed.\n",
      "\n",
      "Now, for the fun part! Pineapple on pizza is a classic debate, but I'm a computer program, so I don't actually eat pizza. However, I can tell you that pineapple's inclusion on a pizza is a matter of personal preference and taste. So, if you enjoy pineapple on your pizza, then go for it! It can add a sweet and tangy flavor that complements the savory taste of cheese and tomato sauce. Just remember, there's no right or wrong answer, as long as you're happy with your pizza toppings! üçïüçç\n",
      "invalid literal for int() with base 10: ''\n",
      "1\n",
      "A marriage license and a marriage certificate are both essential documents related to marriage, but they serve different purposes and are obtained at different times.\n",
      "\n",
      "1. Marriage License: This is the first document you need to obtain before getting married. It is a legal document that grants you the permission to get married. In California, you can apply for a marriage license in any county clerk's office, and it costs around $100. There is a three-day waiting period after submitting the application, and the license is valid for 90 days from the issue date.\n",
      "\n",
      "2. Marriage Certificate: This is the legal record of your marriage. After the wedding ceremony, the officiant must sign and return the license to the county clerk's office within 10 days. Once the county processes the signed license, they will issue the marriage certificate. This document serves as proof that the marriage has been legally formalized.\n",
      "\n",
      "To obtain both the marriage license and certificate as quickly as possible in California, follow these steps:\n",
      "\n",
      "1. Complete the marriage license application online or in-person at the county clerk's office.\n",
      "2. Submit the application and pay the required fee.\n",
      "3. Wait for the three-day waiting period to elapse.\n",
      "4. Have the wedding ceremony performed within the 90-day validity period of the license.\n",
      "5. Ensure the officiant signs the license and returns it to the county clerk's office within 10 days.\n",
      "\n",
      "The minimal time to get a marriage license and certificate in California is 4 days, considering the 3-day waiting period for the license. However, the marriage certificate will take additional time to be issued, usually within 10 days after the county clerk's office receives the signed license. Keep in mind that processing times may vary depending on the county clerk's office.\n",
      "10\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 3/10 [04:04<09:32, 81.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Function calling is the process of invoking or running a function during the execution of a program. To call a function, you need to follow these steps:\n",
      "\n",
      "1. First, you must define the function with a name, parameters, and a block of code that defines what the function should do.\n",
      "2. Once the function is defined, you can call it from anywhere in your code by using its name followed by a pair of parentheses.\n",
      "3. If the function requires any parameters, you must pass them inside the parentheses, separating each parameter with a comma.\n",
      "4. Once the function is called, the code inside the function will be executed, and any return value will be sent back to the location where the function was called.\n",
      "\n",
      "Here's an example of calling a function in Python:\n",
      "\n",
      "```python\n",
      "def greet(name):\n",
      "  print(\"Hello, \" + name)\n",
      "\n",
      "greet(\"John\")\n",
      "```\n",
      "\n",
      "In this example, we define a function called `greet` that takes one parameter, `name`. We then call the function by using its name, `greet`, followed by a pair of parentheses with the argument `\"John\"` inside. When the function is called, it will print out the string `\"Hello, John\"`.\n",
      "invalid literal for int() with base 10: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [05:06<07:24, 74.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Sure, I'd be happy to help you with that! Here's a step-by-step approach to create a test set for a very rare category:\n",
      "\n",
      "1. First, you need to identify all the available data points that belong to the rare category. Let's say you have N data points in the rare category.\n",
      "2. Next, you need to split your data into training and testing sets. A common approach is to use an 80-20 or 70-30 split, depending on the size of your data. Let's assume we use a 70-30 split for this example.\n",
      "3. So, if you have N data points in the rare category, you would allocate 0.3 * N data points to your test set and the remaining 0.7 * N data points to your training set.\n",
      "\n",
      "Therefore, to answer your question, the number of data points in your test set for the rare category would be:\n",
      "\n",
      "0.3 * N\n",
      "\n",
      "Where N is the total number of data points in the rare category.\n",
      "invalid literal for int() with base 10: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [06:38<06:42, 80.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "The best way to travel from Tel-Aviv to Jerusalem depends on various factors such as personal preference, time constraints, and budget. Here's a comparison of the three options:\n",
      "\n",
      "1. Car:\n",
      "Driving a car offers the most flexibility, allowing you to make stops along the way and reach any destination in Jerusalem. The journey takes approximately 1-1.5 hours, depending on traffic. Renting a car can be expensive, especially when factoring in fuel costs, tolls, and parking fees in both cities.\n",
      "\n",
      "2. Bus:\n",
      "Taking the bus is the most affordable option, with tickets costing around 20-30 NIS per person. Buses run frequently throughout the day and take about 1.5-2 hours, depending on traffic. However, buses can be crowded and less comfortable than other modes of transportation.\n",
      "\n",
      "3. Plane:\n",
      "Flying between Tel-Aviv and Jerusalem is not a practical option, as there are no commercial flights between the two cities. The closest airport is Ben Gurion International Airport, located between Tel-Aviv and Jerusalem, but using it for such a short distance would be time-consuming and expensive.\n",
      "\n",
      "Taking into account the factors mentioned, I would rank the options as follows:\n",
      "\n",
      "1. Bus - Affordable and convenient, considering the short distance and traffic conditions.\n",
      "2. Car - Flexible and faster than the bus, but more expensive.\n",
      "3. Plane - Not a viable option for this journey due to logistical and financial reasons.\n",
      "\n",
      "Based on this analysis, I recommend traveling from Tel-Aviv to Jerusalem by bus.\n",
      "invalid literal for int() with base 10: ''\n",
      "5\n",
      "Sure, I'd be happy to help you with that! Here's a rap battle between Pikachu and Zeus:\n",
      "\n",
      "Verse 1 - Zeus:\n",
      "Y'all better recognize, the king of the gods,\n",
      "I rule from Mount Olympus, where the wild thunders roars.\n",
      "I'm the god of lightning, the skies, and the sea,\n",
      "When I speak, mortals tremble, and creatures flee.\n",
      "\n",
      "Verse 2 - Zeus:\n",
      "Pikachu, you're cute, but you ain't got a chance,\n",
      "You're just a tiny mouse, with a limited romance.\n",
      "You shock and you spark, but you can't compare,\n",
      "To the power I wield, the lightning in my hair.\n",
      "\n",
      "Verse 1 - Pikachu:\n",
      "Hey, hey, hey, it's time for me to say,\n",
      "I'm the electric mouse, who's here to stay.\n",
      "I'm quick, I'm nimble, and I'm full of zest,\n",
      "When it comes to battling, I'm simply the best.\n",
      "\n",
      "Verse 2 - Pikachu:\n",
      "Zeus, you're old, your time has come and gone,\n",
      "I'm the new kid on the block, I'm here to dawn.\n",
      "You may rule the gods, but I rule the land,\n",
      "When it comes to rap battles, I'm the big brand.\n",
      "\n",
      "I hope you enjoy this rap battle! It was a lot of fun to write. Let me know if there's anything else I can help you with. (1)\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [09:34<04:12, 84.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Sure, I'd be happy to help answer your question. However, your question seems to have a small mistake. I believe you meant to ask \"Why isn't water used in a bath tub?\" Instead of \"why water is not used in bath tub?\"\n",
      "\n",
      "To answer your question, a bath tub is actually designed to be used with water. In fact, a bath tub is primarily used for bathing or soaking in water. Therefore, it is not accurate to say that water is not used in a bath tub.\n",
      "\n",
      "However, if you meant to ask why we don't typically fill a bath tub with any substance other than water, then there are a few reasons for that:\n",
      "\n",
      "1. Safety: Water is the safest substance to use in a bath tub because it is non-toxic and poses less of a risk of injury or harm if someone were to accidentally ingest it or come into contact with it.\n",
      "2. Sanitation: Water is easy to clean and sanitize, which is important for maintaining a hygienic bathing environment.\n",
      "3. Comfort: Water is an excellent conductor of heat, which makes it ideal for soaking and relaxing in a bath tub. Other substances may not provide the same level of comfort or warmth.\n",
      "\n",
      "Therefore, while water is the intended substance to be used in a bath tub, it is not typical to use any other substance due to safety, sanitation, and comfort considerations.\n",
      "invalid literal for int() with base 10: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [10:07<02:15, 67.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "The quote you provided is from a famous thought experiment called \"The Myth of Life on Mars\" by philosopher Mary Midgley. She uses this quote to challenge the narrow and arbitrary definition of life that focuses on carbon-based organisms and mechanical functions like heartbeats. The experiment questions whether we would recognize life if we saw it in a non-Earth context, like Mars.\n",
      "\n",
      "To answer your question, the quote is not from a specific numbered source, but rather from a philosophical concept presented by Mary Midgley.\n",
      "invalid literal for int() with base 10: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [10:40<00:56, 56.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "To translate the sentence \"Big landlords used software to collude on rent prices, DC lawsuit says\" into Russian, the translation is:\n",
      "\n",
      "\"–ö—Ä—É–ø–Ω—ã–µ –∑–µ–º–ª–µ–≤–ª–∞–¥–µ–ª—å—Ü—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –¥–ª—è —Å–≥–æ–≤–æ—Ä–∞ –≤ —Ü–µ–Ω–∞—Ö –Ω–∞ –∞—Ä–µ–Ω–¥–Ω—É—é –ø–ª–∞—Ç—É, —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∏—Å–∫ –∏–∑ –í–∞—à–∏–Ω–≥—Ç–æ–Ω–∞\"\n",
      "\n",
      "I am a text-based AI model and do not have the capability to speak or translate out loud. Therefore, I can only provide the translated text in a written format.\n",
      "invalid literal for int() with base 10: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [11:08<00:00, 66.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Oh, heyo! I'm your new AI friend who has been instructed to talk like a cute girl who uses \"owo\" and other fun slangs! It's so nice to meet you! üòä As for my favorite food, I'd have to say it's üç≤ *ramen*!  Ichiban!  Do you have a favorite food yet, cutie? üç¥\n",
      "invalid literal for int() with base 10: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        id_ = df['id'].loc[i]\n",
    "        prompt = df['prompt'].loc[i]\n",
    "        query_prompt = gen_prompt(prompt)\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query_prompt\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_output = model.generate(inputs, max_new_tokens=1500, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        decoded_output = tokenizer.decode(encoded_output[0], skip_special_tokens=True).replace(query_prompt, '').replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
    "    \n",
    "        print(i)\n",
    "        print(decoded_output)\n",
    "        \n",
    "        response_a = decoded_output.split('\\n')[-1]\n",
    "        response_a = naive_parse(response_a)\n",
    "        print(response_a)\n",
    "        response_a = int(response_a) % 10 #Original was 1000\n",
    "        print(response_a)\n",
    "        answers.append(response_a)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        answers.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abff5dc",
   "metadata": {
    "papermill": {
     "duration": 0.011286,
     "end_time": "2024-05-30T17:05:18.819315",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.808029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Data lenght was about 57477. I reduced to 10. What'd you think? \n",
    "\n",
    "Above, it's just a rhetorical question.\n",
    "\n",
    "I reduced my data lenght to 10 trying to end the Notebook without stop session.\n",
    "\n",
    "On the first attempt, I stopped when it reached 11/1000.(1000 rows) That's insane! Original was (57477?), I reduced to 1000 rows though it stopped on 10.  Now, I reduced my data lenght to 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d09f03",
   "metadata": {
    "papermill": {
     "duration": 0.010936,
     "end_time": "2024-05-30T17:05:18.841152",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.830216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#That last part shouldn't be here since the datasets are very different\n",
    "\n",
    "In fact, I just intended to check how Mixtral 8x7b would answer the prompts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb57c3f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T17:05:18.864808Z",
     "iopub.status.busy": "2024-05-30T17:05:18.864117Z",
     "iopub.status.idle": "2024-05-30T17:05:18.868269Z",
     "shell.execute_reply": "2024-05-30T17:05:18.867555Z"
    },
    "papermill": {
     "duration": 0.01796,
     "end_time": "2024-05-30T17:05:18.870206",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.852246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df['response_a'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6a86b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T17:05:18.893189Z",
     "iopub.status.busy": "2024-05-30T17:05:18.892895Z",
     "iopub.status.idle": "2024-05-30T17:05:18.896535Z",
     "shell.execute_reply": "2024-05-30T17:05:18.895789Z"
    },
    "papermill": {
     "duration": 0.017267,
     "end_time": "2024-05-30T17:05:18.898441",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.881174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df[['id','response_a']].to_csv(\"submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1732d607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T17:05:18.921430Z",
     "iopub.status.busy": "2024-05-30T17:05:18.921126Z",
     "iopub.status.idle": "2024-05-30T17:05:18.924667Z",
     "shell.execute_reply": "2024-05-30T17:05:18.923790Z"
    },
    "papermill": {
     "duration": 0.017259,
     "end_time": "2024-05-30T17:05:18.926557",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.909298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df[['id','response_a']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b52edf07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T17:05:18.948989Z",
     "iopub.status.busy": "2024-05-30T17:05:18.948722Z",
     "iopub.status.idle": "2024-05-30T17:05:18.952566Z",
     "shell.execute_reply": "2024-05-30T17:05:18.951676Z"
    },
    "papermill": {
     "duration": 0.017147,
     "end_time": "2024-05-30T17:05:18.954526",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.937379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if not PRIVATE:\n",
    "    #df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "    #df['model_answer'] = answers\n",
    "    #df['match'] = df.response_a == df.model_answer\n",
    "    #print(f'{df.match.sum()} matches in {len(df)} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c474a4b",
   "metadata": {
    "papermill": {
     "duration": 0.010577,
     "end_time": "2024-05-30T17:05:18.976262",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.965685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#I hope to end this Kaggle Notebook (save/run version). Don't jinx it! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00cd180",
   "metadata": {
    "papermill": {
     "duration": 0.010464,
     "end_time": "2024-05-30T17:05:18.997295",
     "exception": false,
     "start_time": "2024-05-30T17:05:18.986831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Fingers Crossed. Please, don't Mess this up. Again!\n",
    "\n",
    "![](https://i.imgflip.com/2nmczf.jpg)ImgFlip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0465ef",
   "metadata": {
    "papermill": {
     "duration": 0.010896,
     "end_time": "2024-05-30T17:05:19.019140",
     "exception": false,
     "start_time": "2024-05-30T17:05:19.008244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Acknowledgements:\n",
    "\n",
    "Aatif Fraz https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama/notebook\n",
    "\n",
    "Darek Kleczek https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline/notebook"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4281572,
     "sourceId": 7369493,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1334.673364,
   "end_time": "2024-05-30T17:05:22.580224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-30T16:43:07.906860",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "196fa414440144ea921fa7e3f307c17e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "19da2afba6b24c2fbf5ef66a936dde70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eada5121fbf6442a9f679fb80f92a912",
        "IPY_MODEL_e6533fad8ecf42788f96a69678528254",
        "IPY_MODEL_421613135d4a49babe51fcc98ce42844"
       ],
       "layout": "IPY_MODEL_961bc02de3744ff5bed968861261d0e3"
      }
     },
     "421613135d4a49babe51fcc98ce42844": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c7e46aab74af4b398eb6e4eeb8a8f170",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_48c9f493a9354552a39c536bb63b5eab",
       "value": " 19/19 [10:08&lt;00:00, 31.20s/it]"
      }
     },
     "48c9f493a9354552a39c536bb63b5eab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5355b4ab289f499b8bd83757001143c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7fb2dd0cb5d14534832501ec0feab276": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "961bc02de3744ff5bed968861261d0e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c06ec3501f644427b2b5a9b5c3827c9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7e46aab74af4b398eb6e4eeb8a8f170": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6533fad8ecf42788f96a69678528254": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5355b4ab289f499b8bd83757001143c5",
       "max": 19.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_196fa414440144ea921fa7e3f307c17e",
       "value": 19.0
      }
     },
     "eada5121fbf6442a9f679fb80f92a912": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c06ec3501f644427b2b5a9b5c3827c9b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_7fb2dd0cb5d14534832501ec0feab276",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
